{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0dd755a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\pulij\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "[E050] Can't find model 'en_core_web_sm'. It doesn't seem to be a Python package or a valid path to a data directory.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_17780\\1713377052.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'punkt'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m \u001b[0mnlp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mspacy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"en_core_web_sm\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m sample_paragraph = (\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\spacy\\__init__.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(name, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[0;32m     49\u001b[0m     \u001b[0mRETURNS\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mLanguage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mThe\u001b[0m \u001b[0mloaded\u001b[0m \u001b[0mnlp\u001b[0m \u001b[0mobject\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m     \"\"\"\n\u001b[1;32m---> 51\u001b[1;33m     return util.load_model(\n\u001b[0m\u001b[0;32m     52\u001b[0m         \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m         \u001b[0mvocab\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvocab\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\spacy\\util.py\u001b[0m in \u001b[0;36mload_model\u001b[1;34m(name, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[0;32m    470\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mOLD_MODEL_SHORTCUTS\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    471\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mE941\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfull\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mOLD_MODEL_SHORTCUTS\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[index]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 472\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mE050\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    473\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    474\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mOSError\u001b[0m: [E050] Can't find model 'en_core_web_sm'. It doesn't seem to be a Python package or a valid path to a data directory."
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "import re\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "import spacy\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "sample_paragraph = (\n",
    "    \"Natural Language Processing (NLP) is a fascinating field of Artificial Intelligence. \"\n",
    "    \"It focuses on the interaction between humans and computers using natural language. \"\n",
    "    \"Tokenization is one of the fundamental steps in NLP, where text is broken down into sentences or words.\"\n",
    ")\n",
    "\n",
    "def tokenize_sentences(paragraph):\n",
    "    return sent_tokenize(paragraph)\n",
    "\n",
    "def tokenize_words(paragraph):\n",
    "    return word_tokenize(paragraph)\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"Remove special characters and convert text to lowercase.\"\"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
    "    return text\n",
    "\n",
    "def extract_emails(text):\n",
    "    \"\"\"Extract all email addresses from the given text.\"\"\"\n",
    "    email_pattern = r'[\\w\\.-]+@[\\w\\.-]+\\.\\w{2,}'\n",
    "    return re.findall(email_pattern, text)\n",
    "\n",
    "def fetch_webpage_title(url):\n",
    "    \"\"\"Fetch and print the title of the given webpage.\"\"\"\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        return soup.title.string.strip()\n",
    "    except Exception as e:\n",
    "        return f\"Error fetching webpage title: {e}\"\n",
    "\n",
    "def generate_wordcloud(text, output_file=\"wordcloud.png\"):\n",
    "    \"\"\"Generate a WordCloud from the given text and save it as an image.\"\"\"\n",
    "    wordcloud = WordCloud(width=800, height=400, background_color=\"white\").generate(text)\n",
    "    wordcloud.to_file(output_file)\n",
    "    print(f\"WordCloud saved as {output_file}\")\n",
    "    \n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "def pos_tagging(text):\n",
    "    \"\"\"Perform part-of-speech tagging on the given text.\"\"\"\n",
    "    doc = nlp(text)\n",
    "    pos_tags = [(token.text, token.pos_) for token in doc]\n",
    "    return pos_tags\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    sentences = tokenize_sentences(sample_paragraph)\n",
    "    print(\"Sentences:\")\n",
    "    for idx, sentence in enumerate(sentences, 1):\n",
    "        print(f\"{idx}: {sentence}\")\n",
    "\n",
    "    print(\"\\n\")\n",
    "\n",
    "    words = tokenize_words(sample_paragraph)\n",
    "    print(\"Words:\")\n",
    "    print(words)\n",
    "\n",
    "    print(\"\\n\")\n",
    "\n",
    "    test_text = 'Hello, World! Welcome to NLP 101.'\n",
    "    cleaned_text = clean_text(test_text)\n",
    "    print(\"Cleaned Text:\")\n",
    "    print(cleaned_text)\n",
    "\n",
    "    print(\"\\n\")\n",
    "\n",
    "    email_text = 'Contact us at support@example.com and sales@example.org.'\n",
    "    emails = extract_emails(email_text)\n",
    "    print(\"Extracted Emails:\")\n",
    "    print(emails)\n",
    "\n",
    "    print(\"\\n\")\n",
    "\n",
    "    url = 'https://example.com'\n",
    "    webpage_title = fetch_webpage_title(url)\n",
    "    print(\"Webpage Title:\")\n",
    "    print(webpage_title)\n",
    "\n",
    "    print(\"\\n\")\n",
    "\n",
    "    wordcloud_text = 'data science machine learning artificial intelligence'\n",
    "    generate_wordcloud(wordcloud_text)\n",
    "\n",
    "    print(\"\\n\")\n",
    "\n",
    "    pos_text = 'NLP is amazing and fun to learn.'\n",
    "    pos_tags = pos_tagging(pos_text)\n",
    "    print(\"POS Tags:\")\n",
    "    for word, tag in pos_tags:\n",
    "        print(f\"{word}: {tag}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2897af3e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
